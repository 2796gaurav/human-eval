{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab-human_eval-codex",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVn/ZhLG/swI6mvzNGHvt4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2796gaurav/human-eval/blob/master/colab_human_eval_codex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLbxcauyhmtt"
      },
      "source": [
        "# Human Eval in Google Colab\n",
        "\n",
        "[Original Openai github source](https://github.com/openai/human-eval)\n",
        "\n",
        "[Paper](https://arxiv.org/pdf/2107.03374.pdf)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epEQJik7hr_Y"
      },
      "source": [
        "## download code and dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XfQTi37Uzl3",
        "outputId": "a5df9ff2-3937-4e00-a559-7c56d794f4a2"
      },
      "source": [
        "!git clone https://github.com/openai/human-eval.git\n",
        "!pip install -e human-eval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'human-eval'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 25 (delta 6), reused 21 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (25/25), done.\n",
            "Obtaining file:///content/human-eval\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from human-eval==1.0) (4.41.1)\n",
            "Collecting fire\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from human-eval==1.0) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire->human-eval==1.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->human-eval==1.0) (1.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=341866823281cf2fd1565ff953e29a87447b823ca929394b9d263c77bf347735\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built fire\n",
            "Installing collected packages: fire, human-eval\n",
            "  Running setup.py develop for human-eval\n",
            "Successfully installed fire-0.4.0 human-eval-1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu7y9jiSU01u",
        "outputId": "e2aae2d1-04f2-4336-e34a-69cb0d0cf05d"
      },
      "source": [
        "cd human-eval/human_eval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/human-eval/human_eval\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQD-qZHuhvHC"
      },
      "source": [
        "## Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSbhMN-VU5NS"
      },
      "source": [
        "from data import write_jsonl, read_problems"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN-5SmNlVDMS"
      },
      "source": [
        "problems = read_problems()\n",
        "\n",
        "num_samples_per_task = 200\n",
        "generate_one_completion = {\"task_id\": \"test/0\", \n",
        "                           \"prompt\": \"def return1():\\n\", \n",
        "                           \"canonical_solution\": \"    return 1\", \n",
        "                           \"test\": \"def check(candidate):\\n    assert candidate() == 1\", \n",
        "                           \"entry_point\": \"return1\"}\n",
        "samples = [\n",
        "    generate_one_completion\n",
        "    for task_id in problems\n",
        "    for _ in range(num_samples_per_task)\n",
        "]\n",
        "\n",
        "write_jsonl(\"samples.jsonl\", samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te3bLldfUdMb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f49d88b8-75a1-4dcb-82a2-88363d1d9e5d"
      },
      "source": [
        "len(problems)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "164"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wSW0OrPT-OV",
        "outputId": "721d42d1-9927-4e22-fb68-4e3b28b3cbd2"
      },
      "source": [
        "problems['HumanEval/1']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'canonical_solution': \"    result = []\\n    current_string = []\\n    current_depth = 0\\n\\n    for c in paren_string:\\n        if c == '(':\\n            current_depth += 1\\n            current_string.append(c)\\n        elif c == ')':\\n            current_depth -= 1\\n            current_string.append(c)\\n\\n            if current_depth == 0:\\n                result.append(''.join(current_string))\\n                current_string.clear()\\n\\n    return result\\n\",\n",
              " 'entry_point': 'separate_paren_groups',\n",
              " 'prompt': 'from typing import List\\n\\n\\ndef separate_paren_groups(paren_string: str) -> List[str]:\\n    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\\n    separate those group into separate strings and return the list of those.\\n    Separate groups are balanced (each open brace is properly closed) and not nested within each other\\n    Ignore any spaces in the input string.\\n    >>> separate_paren_groups(\\'( ) (( )) (( )( ))\\')\\n    [\\'()\\', \\'(())\\', \\'(()())\\']\\n    \"\"\"\\n',\n",
              " 'task_id': 'HumanEval/1',\n",
              " 'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('(()()) ((())) () ((())()())') == [\\n        '(()())', '((()))', '()', '((())()())'\\n    ]\\n    assert candidate('() (()) ((())) (((())))') == [\\n        '()', '(())', '((()))', '(((())))'\\n    ]\\n    assert candidate('(()(())((())))') == [\\n        '(()(())((())))'\\n    ]\\n    assert candidate('( ) (( )) (( )( ))') == ['()', '(())', '(()())']\\n\"}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oHWO0KGSpJs"
      },
      "source": [
        "## problems given by openai\n",
        "\n",
        "import json\n",
        "with open('problems.json', 'w') as fp:\n",
        "    json.dump(problems, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFboHY6rhycv"
      },
      "source": [
        "## update execution.py\n",
        "### Note\n",
        "**This program exists to run untrusted model-generated code. Users are strongly encouraged not to do so outside of a robust security sandbox. The execution call in execution.py is deliberately commented out to ensure users read this disclaimer before running code in a potentially unsafe manner. See the comment in execution.py for more information and instructions.**\n",
        "\n",
        "To continue, you need to manually update `execution.py`, specifically uncomment `exec(check_program, exec_globals)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSPtSzyFVH2V",
        "outputId": "9faf58ba-8edd-45cf-b0e3-3db274709cb5"
      },
      "source": [
        "!python evaluate_functional_correctness.py ../data/example_samples.jsonl --problem_file=../data/example_problem.jsonl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading samples...\n",
            "\r0it [00:00, ?it/s]\r6it [00:00, 4154.83it/s]\n",
            "Running test suites...\n",
            "100% 6/6 [00:03<00:00,  1.95it/s]\n",
            "Writing results to ../data/example_samples.jsonl_results.jsonl...\n",
            "100% 6/6 [00:00<00:00, 11853.90it/s]\n",
            "{'pass@1': 0.4999999999999999}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}