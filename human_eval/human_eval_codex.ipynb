{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLbxcauyhmtt"
   },
   "source": [
    "# Human Eval in Google Colab\n",
    "\n",
    "[Original Openai github source](https://github.com/openai/human-eval)\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/2107.03374.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQD-qZHuhvHC"
   },
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FSbhMN-VU5NS"
   },
   "outputs": [],
   "source": [
    "from data import write_jsonl, read_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tN-5SmNlVDMS"
   },
   "outputs": [],
   "source": [
    "problems = read_problems()\n",
    "\n",
    "num_samples_per_task = 200\n",
    "generate_one_completion = {\"task_id\": \"test/0\", \n",
    "                           \"prompt\": \"def return1():\\n\", \n",
    "                           \"canonical_solution\": \"    return 1\", \n",
    "                           \"test\": \"def check(candidate):\\n    assert candidate() == 1\", \n",
    "                           \"entry_point\": \"return1\"}\n",
    "samples = [\n",
    "    generate_one_completion\n",
    "    for task_id in problems\n",
    "    for _ in range(num_samples_per_task)\n",
    "]\n",
    "\n",
    "write_jsonl(\"samples.jsonl\", samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Te3bLldfUdMb",
    "outputId": "f49d88b8-75a1-4dcb-82a2-88363d1d9e5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1wSW0OrPT-OV",
    "outputId": "721d42d1-9927-4e22-fb68-4e3b28b3cbd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_id': 'HumanEval/1',\n",
       " 'prompt': 'from typing import List\\n\\n\\ndef separate_paren_groups(paren_string: str) -> List[str]:\\n    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\\n    separate those group into separate strings and return the list of those.\\n    Separate groups are balanced (each open brace is properly closed) and not nested within each other\\n    Ignore any spaces in the input string.\\n    >>> separate_paren_groups(\\'( ) (( )) (( )( ))\\')\\n    [\\'()\\', \\'(())\\', \\'(()())\\']\\n    \"\"\"\\n',\n",
       " 'entry_point': 'separate_paren_groups',\n",
       " 'canonical_solution': \"    result = []\\n    current_string = []\\n    current_depth = 0\\n\\n    for c in paren_string:\\n        if c == '(':\\n            current_depth += 1\\n            current_string.append(c)\\n        elif c == ')':\\n            current_depth -= 1\\n            current_string.append(c)\\n\\n            if current_depth == 0:\\n                result.append(''.join(current_string))\\n                current_string.clear()\\n\\n    return result\\n\",\n",
       " 'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('(()()) ((())) () ((())()())') == [\\n        '(()())', '((()))', '()', '((())()())'\\n    ]\\n    assert candidate('() (()) ((())) (((())))') == [\\n        '()', '(())', '((()))', '(((())))'\\n    ]\\n    assert candidate('(()(())((())))') == [\\n        '(()(())((())))'\\n    ]\\n    assert candidate('( ) (( )) (( )( ))') == ['()', '(())', '(()())']\\n\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problems['HumanEval/1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0oHWO0KGSpJs"
   },
   "outputs": [],
   "source": [
    "## problems given by openai\n",
    "\n",
    "import json\n",
    "with open('problems.json', 'w') as fp:\n",
    "    json.dump(problems, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFboHY6rhycv"
   },
   "source": [
    "## update execution.py\n",
    "### Note\n",
    "**This program exists to run untrusted model-generated code. Users are strongly encouraged not to do so outside of a robust security sandbox. The execution call in execution.py is deliberately commented out to ensure users read this disclaimer before running code in a potentially unsafe manner. See the comment in execution.py for more information and instructions.**\n",
    "\n",
    "To continue, you need to manually update `execution.py`, specifically uncomment `exec(check_program, exec_globals)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSPtSzyFVH2V",
    "outputId": "9faf58ba-8edd-45cf-b0e3-3db274709cb5"
   },
   "outputs": [],
   "source": [
    "!python evaluate_functional_correctness.py ../data/example_samples.jsonl --problem_file=../data/example_problem.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "human_eval-codex",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
